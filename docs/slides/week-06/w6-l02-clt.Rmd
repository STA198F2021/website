---
title: "Distribution of the Sample Mean"
subtitle: "<br><br> Introduction to Global Health Data Science"
author: "[Back to Website](https://sta198f2021.github.io/website/)"
date: "<br> Prof. Amy Herring"
output:
  xaringan::moon_reader:
    css: 
      - css/xaringan-themer.css
      - css/slides.css
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r child = "../setup.Rmd"}
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      comment = "#>", highlight = TRUE,
                      fig.align = "center")
```

## Main ideas

- Understand the Central Limit Theorem (CLT) and how to use the result

- Create confidence intervals for the population mean using a CLT-based
  approach
  
- Create confidence intervals for the population proportion using a CLT-based
  approach
  
---
  
### Packages

```{r packages}
library(tidyverse)
library(infer)
```

---

### Central Limit Theorem

The *central limit theorem* says that for any distribution with a well-defined mean and variance, the distribution of *means* for a sample of size $n$ is approximately normal. This result has strong implications in many areas of statistics, including in construction of confidence intervals and in hypothesis testing.

---

### Population and Samples

.pull-left[
- Statistical inference is the act of generalizing from a sample to a population with an estimated degree of uncertainty
- We want to know about *parameters* in a population
- We calculate *statistics* in our sample to learn about parameters
]
.pull-right[

```{r echo=FALSE}
knitr::include_graphics("img/popsample.png")
```
]

---

### Parameters and Statistics 

It is imperative to understand the difference between statistics and parameters.

|   | **Parameters** | **Statistics** |
|:----|:------:|:------:|
| Source | Population | Sample |
| Calculated? | No | Yes |
| Example notation | $\mu, \sigma, \pi$ | $\overline{x}, s, p, \hat{\mu}, \hat{\sigma}, \hat{\pi}$ |

---

### Parameters and Statistics

.pull-left[
```{r echo=FALSE}
knitr::include_graphics("img/plato.png")
```
]
.pull-right[
Still confused about parameters and statistics? Let's talk Plato instead. Plato presented his famous **Allegory of
the Cave** in *The Republic*. It may be
helpful think of parameters as Platonic
forms and the statistics we calculate as
the shadows on the wall of the cave.
Here is a short video clip describing this
allegory: [Allegory of the Cave
Claymation](https://www.youtube.com/watch?v=E4XXItJYFKA)
]

---

### Sampling Distribution of the Mean

Suppose we have an infinitely large population and do the following.

1. Take a sample of size $n$ and calculate its mean $\overline{x}_1$
2. Take a second sample of the same size and calculate its mean $\overline{x}_2$
3. Repeat this many times to get a dataset of sample means $\overline{x}_1, \overline{x}_2, \ldots$

What is the distribution of the *statistics* $\overline{x}_1, \overline{x}_2, \overline{x}_3, \ldots$?

---

### Test case: Binomial data

In a seminar on the COVID epidemic in India, Professor Bhramar Mukherjee noted that 67% of the population there has been infected.  Define the variable $X$ to take value 1 if a randomly sampled Indian resident has been infected, and let it be 0 otherwise.

- The random variable $X$ is Bernoulli. A Bernoulli random variable has mean $\pi$ and variance $\pi(1-\pi)$. Here $\pi=0.67$.
- If we take a random sample, the average $\overline{x}=\hat{\pi}$ is an *estimate* of this probability (it's just the fraction of 1's)
- If we take repeated samples of Indian residents and compute the proportion with prior infection in each, what values will we see?  Will we get 0.67 each time?

---

### Simulation: Prior Infections

Recall we did this back when we were graphing categorical data! Here, we generate one thousand samples from a Binomial with $\pi$=0.67 and different values of $n$, and then we graph the result.



```{r genbinom}
binomdata = tibble(
  X=c(rbinom(n=1000,10,.67),
      rbinom(n=1000,100,.67),
      rbinom(n=1000,1000,.67)),
  N=c(rep(10,1000),rep(100,1000),
      rep(1000,1000)),pihat=X/N)
```
This code generates 1000 samples from a binomial $n=10$, $\pi=0.67$ distribution, 1000 samples from a binomial $n=100$, $\pi=0.67$ distribution, and 1000 samples from a binomial $n=1000$, $\pi=0.67$ distribution. The vector $X$ contains the count $x$ in each sample, and the vector $N$ contains the sample size (so is 10 for the first 1000 entries, 100 for the second 1000, and so forth). We estimate $\pi$ as $\hat{\pi}=\frac{x}{n}$ in each sample.

---

.pull-left[
```{r head}
head(binomdata)
```
]

.pull-right[
```{r tail}
tail(binomdata)
```
]



---

```{r plotbinom, eval=FALSE}
binomdata %>%
ggplot(aes(x = pihat, fill = as.factor(N))) + 
  geom_density() +
  labs(x = "Mean Fraction with Prior Infection",
       title = "1000 Samples of Size N with pi=0.67",
       fill = "Sample Size")
```

How does the variability of the sampling distribution depend on the size of our random samples over which the mean is calculated (the binomial "n")?

---

### Plotted samples

```{r plotbinom2, echo=FALSE}
binomdata %>%
ggplot(aes(x = pihat, fill = as.factor(N))) + 
  geom_density() +
  labs(x = "Mean Fraction with Prior Infection",
       title = "1000 Samples of Size N with pi=0.67",
       fill = "Sample Size")
```
---

### Central Limit Theorem

The *Central Limit Theorem* says that for a population with mean $\mu$ and standard deviation $\sigma$, the important three properties of the distribution of sample averages $\bar{x}$ hold:
- The mean of the sampling distribution is identical to the population mean $\mu$.
- The standard deviation of the distribution of the sample averages is $\frac{\sigma}{\sqrt{n}}$, called the *standard error* of the mean.
- For $n$ large enough (in the limit as $n \rightarrow \infty$), the shape of the sampling distribution is approximately normal (Gaussian)

[See the CLT in Action!](http://demonstrations.wolfram.com/SamplingDistributionOfTheSampleMean/)

Note for our Bernoulli example, $\mu=\pi$ and $\sigma=\sqrt{\pi(1-\pi)}$


---

### How Large is Large Enough for $n$?

For binomial data, one rule of thumb is that you want the 
sample size $n$ to be large enough that $n \pi>10$ and $n(1- \pi)>10$.

---

### Wait, that variable was binomial!

The Central Limit Theorem tells us that the *sample averages* are normally distributed if we have enough data.  This result holds even if our original variables (here, we started with 0/1 indicators of prior infection) are not normally distributed.

---


### Distribution of the sample mean



Knowing the distribution of the sample statistic $\bar{X}$ can help us

- estimate a population parameter as point estimate $\pm$ margin of error, where 
  the margin of error is comprised of a measure of how confident we want to be 
  and the sample statistic's variability.

- test for a population parameter by evaluating how likely it is to obtain the
  observed sample statistic when assuming that the null hypothesis is true, as 
  this probability will depend on the sampling distribution's variability.
  
---

## Normal Distribution

When necessary conditions are met, we can also use inference methods based on the 
CLT. Then the CLT tells us that $\bar{X}$ approximately has the distribution 
$N\left(\mu, \left(\sigma/\sqrt{n}\right)^2\right)$. That is,

$$Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$

   