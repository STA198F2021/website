---
title: "Hypothesis Testing"
subtitle: "<br><br> Introduction to Global Health Data Science"
author: "[Back to Website](https://sta198f2021.github.io/website/)"
date: "<br> Prof. Amy Herring"
output:
  xaringan::moon_reader:
    css: 
      - css/xaringan-themer.css
      - css/slides.css
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r child = "../setup.Rmd"}
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      comment = "#>", highlight = TRUE,
                      fig.align = "center")
library(emo) #for emojis
```

## Main ideas

-- Learn one-sample test of mean

-- Learn one-sample test of proportions


---
  
### Packages

```{r packages}
library(tidyverse)
library(infer)
library(readxl)
```

---


### Hypothesis Testing Framework


Steps in hypothesis testing:

- Start with two claims about the population (often about the value of a population parameter or about some potential association between two variables in the population).  Call them claim 1 and claim 2.

- Choose a sampling strategy, draft an analysis plan, collect data, and summarize data

- Figure out how likely it is to see data like what we got, if claim 1 is true.

- If our data would have been unlikely if claim 1 were true, then we reject claim 1 and deem claim 2 worthy of further study.  Otherwise, we cannot reject claim 1.

---




### Hypothesis Testing About the Mean

1. Hypothesize a value, $\mu_0$, and set up $H_0$ and $H_A$

2. Take a random sample of size $n$ and calculate summary statistics (e.g., sample mean and variance)

3. Is it **likely** that the sample, or one even more extreme, came from a population with mean $\mu_0$?

4. Draw conclusions

---

### Null and Alternative Hypotheses About the Mean

We set up the hypotheses to cover *all* the possibilities for $\mu$ and consider three possibilities.

|  | $H_0$  | $H_A$|
|:------|:------:|:---------:|
| Two-sided | $\mu=\mu_0$ | $\mu \neq \mu_0$ |
| One-sided | $\mu>\mu_0$ | $\mu \leq \mu_0$ |
| One-sided | $\mu<\mu_0$ | $\mu \geq \mu_0$ |

One-sided tests are pretty rare. 

---

### Null and Alternative Hypotheses About a Proportion
We set up the hypotheses to cover *all* the possibilities for $\pi$ and consider three possibilities.

|  | $H_0$  | $H_A$|
|:------|:------:|:---------:|
| Two-sided | $\pi=\pi_0$ | $\pi \neq \pi_0$ |
| One-sided | $\pi>\pi_0$ | $\pi \leq \pi_0$ |
| One-sided | $\pi<\pi_0$ | $\pi \geq \pi_0$ |

One-sided tests are pretty rare. 

---

### Two-Sided Hypothesis Tests

To conduct the hypothesis test, we use what we learned about the sampling distribution of the sample mean $\bar{X}$.  If the underlying population is normally distributed (or $n$ is pretty large), then the random variable $$t=\frac{\bar{X}-\mu_0}{\frac{s}{\sqrt{n}}}$$ has a $t_{n-1}$ distribution, and we can use a *t-test* of our hypothesis by using software (R `t.test`).  This test is called the *one sample t-test*.

For a test of a proportion, where we don't have to estimate a separate variance parameter, we could use a z-statistic and compare to a $N(0,1)$ distribution or even better the exact binomial distribution itself (R `binom.test`).

---

### Behind the Scenes

You will almost always use software to calculate the test statistic and p-value, but it is important to understand the big concepts behind the scenes.

First, think about our test statistic $$t=\frac{\bar{X}-\mu_0}{\frac{s}{\sqrt{n}}}.$$

- $\bar{X}-\mu_0$ makes sense, because we want to look at how far our sample mean is from the hypothesized population mean

- Whether $\bar{X}-\mu_0$ is big depends on the standard deviation.  For example, a difference of $\bar{X}-\mu_0=1$ is a small difference if we are looking at weight in g but huge for height in m.  This is why we standardize the difference by dividing by the *estimated* SD of the mean, so $t$ is a measure of how many SDs apart $\mu_0$ and $\bar{X}$ are from each other

---

### Behind the Scenes

Think about our test statistic $$t=\frac{\bar{X}-\mu_0}{\frac{s}{\sqrt{n}}}.$$

- When our test statistic $t$ is big and our data are approximately normal (or $n$ is large), our data are not consistent with data from a population with mean $\mu_0$
  - Problem: our data might be unlikely under $H_A$ as well (our test will not tell us that!)
  
- When our test statistic $t$  is small (and our data are approximately normal or $n$ is large), then our data do not refute the null hypothesis.

---

### Getting the p-value: 2-sided test

.pull-left-narrow[

|  | $H_0$  | $H_A$|
|:------|:------:|:---------:|
| Two-sided | $\mu=\mu_0$ | $\mu \neq \mu_0$ |
| One-sided | $\mu>\mu_0$ | $\mu \leq \mu_0$ |
| One-sided | $\mu<\mu_0$ | $\mu \geq \mu_0$ |



]
.pull-right-wide[

For a *two-sided* test, the p-value is the probability of seeing a t statistic with absolute value as big as, or larger than, what we saw in our data.

```{r echo=FALSE,out.width="50%"}
knitr::include_graphics("img/tdisttwotail.png")
```
]

---



### Getting the p-value: 1-sided test

.pull-left-narrow[

|  | $H_0$  | $H_A$|
|:------|:------:|:---------:|
| Two-sided | $\mu=\mu_0$ | $\mu \neq \mu_0$ |
| One-sided | $\mu>\mu_0$ | $\mu \leq \mu_0$ |
| One-sided | $\mu<\mu_0$ | $\mu \geq \mu_0$ |



]
.pull-right-wide[

For a *one-sided* test with $H_0: \mu > \mu_0$, the p-value is the probability of seeing a t statistic with value the same size, or smaller than, what we saw in our data.

```{r echo=FALSE,out.width="50%"}
knitr::include_graphics("img/tdistlefttail.png")
```
]

---



### Getting the p-value: 1-sided test

.pull-left-narrow[

|  | $H_0$  | $H_A$|
|:------|:------:|:---------:|
| Two-sided | $\mu=\mu_0$ | $\mu \neq \mu_0$ |
| One-sided | $\mu>\mu_0$ | $\mu \leq \mu_0$ |
| One-sided | $\mu<\mu_0$ | $\mu \geq \mu_0$ |



]
.pull-right-wide[

For a *one-sided* test with $H_0: \mu < \mu_0$, the p-value is the probability of seeing a t statistic with value the same size, or larger than, what we saw in our data.

```{r echo=FALSE,out.width="50%"}
knitr::include_graphics("img/tdistrighttail.png")
```
]
---

### Warning about 1-sided tests

The choice between a one-sided and a two-sided test can be highly controversial because a one-sided test will have a p-value that is half that of the corresponding two-sided test, due to the symmetry of the $t$ distribution.  Sometimes a scientist will (unethically) choose a one-sided test on nonscientific grounds.  To protect against this, some journal editors are extremely reluctant to publish studies using one-sided tests.

---

### Are One-Sided Tests Underutilized?

Usually, you have a pretty good idea what will happen (you do have to get research funding using some justification!).  In this case, why not always do one-sided tests?

---

### CAST Study

Example:  CAST (Cardiac Arrhythmia Suppression Trial)
- New generation of antiarrhythmic agents strongly believed to have fewer side effects with much greater efficacy
- Due to strength of this belief, one-sided test was selected

- Recruitment was difficult because many physicians refused to randomize their patients when chance of NOT getting the new drugs was 50%
  - The trial showed new generation drugs associated with 4x the mortality as status quo
  - P-value was 0.0003 in the wrong direction (i.e., new drug worse), but with a one-sided test that direction is part of the null hypothesis of "status quo"
  - Fortunately the Data Safety and Monitoring Board stopped the trial quickly

---

# P-Values

---

### P-value Defined

The *p-value* is the conditional probability, under the assumption of no effect or no difference, $H_0$, of obtaining a result equal to or more extreme than what was actually observed.

---

### Incorrect P-Value Interpretations

Many researchers think a p-value of 0.05 means the null hypothesis has a probability of only 5%.  This is wrong!

---

### Incorrect P-Value Interpretations

Many researchers will see $p=0.05$ and state that there is a 95% or greater chance that the null hypothesis is incorrect.  This is wrong!

A p-value is calculated *assuming* that $H_0$ is true.  It cannot be used to tell us how likely it is that assumption is correct.

---

### Hypothesis Testing Paradigm

The paradigm of hypothesis testing tries to limit the overall number of incorrect decisions over the long run.  While we of course want to know if any one study is showing us something real, or a type I or type II error, the paradigm does not give us the tools to determine this.

---

### What DO You Do with a P-value, Then?

Ideally, you evaluate a p-value in light of other information, such as a proposed biological mechanism, supporting evidence in the literature, size and quality of the study, and size of the purported effect.

In addition, when you interpret a p-value, be sure you are aware of other important factors that can inflate the false positive rate, e.g. multiple tests, "hidden" multiple tests (e.g., testing most appropriate form of covariate or for an interaction or other model selection procedures), changes to sample size, etc.

---

### I'm Nervous about P-values.  What Alternatives are There?

- Many researchers prefer confidence intervals, which better represent the range of effects that are "compatible with the data"

- They are sometimes used as a hypothesis test (i.e., reporting results as significant when confidence interval does not include null value) and share many of the properties of p-values we have discussed

- Like p-values, confidence intervals do not offer a mechanism to unite external evidence with that provided by the study at hand

- *Bayesian methods* can be used to incorporate current study results with prior knowledge in order to provide a probability a hypothesis is true conditional on the current data (Bayes' theorem can get us what we want to know!)
---
### Correspondence between Hypothesis Tests and Confidence Intervals

We can also examine a confidence interval to decide whether a proposed value for the population mean is reasonable.

Suppose we are testing $H_0:  \mu=\mu_0$ versus $H_A:  \mu \neq \mu_0$ using a significance level of $\alpha=0.05$.   An alternative way to do this is to construct a 95% confidence interval for $\mu$ and use the following decision rule.
- If $\mu_0$ falls outside the confidence interval, reject $H_0$
- If $\mu_0$ falls inside the confidence interval, do not reject $H_0$

We may not want to make a binary decision but instead present a confidence interval and provide discussion

---


### Lead in Flint, MI

From April 25, 2014 to October 15, 2015, the water supply source for Flint, MI was switched to the Flint River from the Detroit water system. Without corrosion inhibitors, the Flint River water, which is high in chloride, caused lead from aging pipes to leach into the water supply. We have data from Flint collected as part of a citizen-science project involving Virginia Tech researchers.

```{r echo=FALSE}
knitr::include_graphics("img/flintdetroit.jpg")
```

---

### Back to Flint

Let's conduct an $\alpha=0.05$ t-test of $H_0: \mu=15$ versus $H_A: \mu \neq 15$.

```{r getdata}
flint=read_excel("data/Flint-Samples.xlsx",sheet=1)
flint=rename(flint, "Pb_initial"="Pb Bottle 1 (ppb) - First Draw")
t.test(flint$Pb_initial, mu=15)
```

The null value 15 is not in our CI, and the p-value is quite small.  We have evidence to reject the null hypothesis, and fortunately the mean lead level appears to be less than 15.

---

# Bootstrapping

---

### Bootstrapping

We can use a bootstrap procedure in hypothesis testing, similar to the one we used to calculate a confidence interval.  

First, let's simulate the null distribution -- what we expect to see if the mean is 15.

```{r simulatenull}
set.seed(1234)
null_dist <- flint %>%
  specify(response = Pb_initial) %>%
  hypothesize(null = "point", mu = 15) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

---

### Visualize Null Distribution

```{r visualnull}
visualize(null_dist) +
  labs(x = "Sample means", y = "Count", 
       title = "Simulated null distribution")

```


---

Let's add our sample mean to this distribution and shade the p-value.

```{r sampmean, out.width="40%"}
x_bar <- flint %>%
  summarize(mean_Pb=mean(Pb_initial))

visualize(null_dist) +
  shade_p_value(obs_stat = x_bar, 
                direction = "two-sided") +
  labs(x = "Sample means", y = "Count")
```

 
Notice our sample mean of 10.65 is below the mass of this distribution. 

What is your conclusion?

---

Now suppose instead we wanted to test $H_0: \mu=10$. In this case, we would get a different result:

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "null10", echo = FALSE, warning = FALSE, out.width = "50%"}
```
Now the p-value, corresponding to the shaded area, is quite large.
]
.panel[.panel-name[Code]

```{r null10, fig.show = "hide"}
null_dist_10 <- flint %>%
  specify(response = Pb_initial) %>%
  hypothesize(null = "point", mu = 10) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

visualize(null_dist_10) +
  shade_p_value(obs_stat = x_bar, 
                direction = "two-sided") +
  labs(x = "Sample means", y = "Count")

```
]
]

---

### Testing proportions

Suppose we wish to test the null hypothesis that 20% of Flint households have water with lead levels over 15 ppb, versus the alternative that this percentage is not 20%.

While we could use the Central Limit Theorem here, we actually have an even better option in R -- we can carry out this test based on the (exact) binomial distribution. With the binomial distribution, we don't have to worry about the sample size or rules of thumb for our test to be accurate. The Central Limit Theorem-based methods became very popular when computers were less powerful, but now we can use *exact* tests much more easily, even when large combinatorial calculations are involved.

This exact binomial test works in the same manner as the test we used earlier for fairness in hiring -- but we can avoid doing it by hand using `binom.test` (you can go back and check that result!).

---



```{r proptest}
flintprop<-flint %>%
  mutate(Pbover15 = Pb_initial > 15) %>%
  summarize(countover15 = sum(Pbover15=="TRUE"),
            n.lead = n(),phat=countover15/n.lead) 
binom.test(flintprop$countover15,flintprop$n.lead,p=0.20)
```
---

We can also explore this using simulation.  For binary data, we can draw random samples from a Bernoulli distribution with probability of success equal to the value of `p` given in the code.



.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "simbinom", echo = FALSE, warning = FALSE, out.width = "50%"}
```

]
.panel[.panel-name[Code]

```{r simbinom, fig.show = "hide"}
null_dist_20 <- flint %>%
  mutate(Pbover15 = Pb_initial > 15) %>%
  specify(response = Pbover15, success= "TRUE") %>%
  hypothesize(null = "point", p = 0.2) %>%
  generate(reps = 10000, type = "draw") %>%
  calculate(stat = "prop")

visualize(null_dist_20) +
  shade_p_value(obs_stat = flintprop$phat, 
                direction = "two-sided") +
  labs(x = "Sample proportions", y = "Count")

```
]
]

---

# Try it out!

---

### Flint Lead

Public health officials often recommend that residents who have lead in their drinking water run the water for 45 seconds to 2 minutes (called "flushing") to remove as much lead from the drinking water as possible.

In our data, we have two measures taken after flushing the pipes -- one after 45 seconds, and one after two minutes.

The *paired t-test* is useful in situations like this one, where we have an initial measure as well as follow-up measures.  We can simply subtract two measures to obtain a "difference" measure and then carry out a regular one-sample t-test.

---

Create two new variables, calculated as (1) initial lead concentration minus the lead concentration after 45 seconds of flushing, and (2) initial lead concentration minus the lead concentration after 2 minutes of flushing.  Then conduct two hypothesis tests: in each case, your goal is to evaluate whether the *paired differences* are 0.   Interpret your results in terms of lead levels before and after flushing. Include useful graphical displays to supplement your test and interpretation.

---




